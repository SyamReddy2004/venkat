# ğŸ§  CIFAR-10 MLOps Pipeline

> **Multiclass image classification on CIFAR-10 with TensorFlow â€” experiment
> tracking with MLflow â€” automated training orchestration with Apache Airflow.**

---

## ğŸ—‚ Project Structure

```
cifar10-mlops/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml              # Central configuration (data, model, training, MLflow, Airflow)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config_loader.py         # YAML config loader
â”‚   â”œâ”€â”€ data_preprocessing.py    # CIFAR-10 loading, normalisation, augmentation, tf.data pipelines
â”‚   â”œâ”€â”€ model.py                 # 3 architectures + callback factory
â”‚   â”œâ”€â”€ train.py                 # Training loop with full MLflow tracking
â”‚   â”œâ”€â”€ evaluate.py              # Comprehensive evaluation (ROC, PR curves, confusion matrix)
â”‚   â””â”€â”€ model_registry.py       # Register, compare, and promote models
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ cifar10_training_pipeline.py   # Airflow DAG (7 tasks, TaskFlow API)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_training.py          # Standalone CLI training
â”‚   â”œâ”€â”€ run_evaluation.py        # Standalone CLI evaluation
â”‚   â”œâ”€â”€ setup_mlflow.py          # One-time MLflow initialisation
â”‚   â”œâ”€â”€ setup_airflow.py         # One-time Airflow bootstrap
â”‚   â”œâ”€â”€ compare_runs.py          # Compare MLflow runs
â”‚   â””â”€â”€ start_services.sh        # Launch all services
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_pipeline.py         # pytest suite (config, data, model, training)
â”œâ”€â”€ models/                      # Saved model checkpoints (git-ignored)
â”œâ”€â”€ logs/                        # TensorBoard logs + evaluation plots (git-ignored)
â”œâ”€â”€ mlruns/                      # MLflow artefact store (git-ignored)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### 1 â€” Create a virtual environment

```bash
cd cifar10-mlops
python -m venv .venv
source .venv/bin/activate        # macOS / Linux
# .venv\Scripts\activate.bat     # Windows
```

### 2 â€” Install dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

> **Apple Silicon (M-series)** â€” install the Metal-optimised TensorFlow instead:
> ```bash
> pip install tensorflow-macos tensorflow-metal
> ```

### 3 â€” Initialize MLflow

```bash
python scripts/setup_mlflow.py
```

### 4 â€” Start MLflow UI (keep this terminal open)

```bash
mlflow ui --port 5000 --backend-store-uri file://$(pwd)/mlruns
# â†’ open http://localhost:5000
```

### 5 â€” Run training (standalone, fastest way to test)

```bash
python scripts/run_training.py
# or with overrides:
python scripts/run_training.py --arch simple_cnn --epochs 5 --register
```

---

## ğŸ— Model Architectures

| Key | Description | Params |
|-----|-------------|--------|
| `simple_cnn` | 3-block VGG-style CNN (fast baseline) | ~1.2 M |
| `resnet_custom` â­ | 4-stage custom ResNet with pre-activation residual blocks | ~6.6 M |
| `efficientnet_transfer` | EfficientNetB0 (frozen) + custom head, fine-tunable | ~4.3 M |

Switch architecture in `config/config.yaml`:

```yaml
model:
  architecture: resnet_custom   # simple_cnn | resnet_custom | efficientnet_transfer
```

---

## ğŸ“Š What Gets Tracked in MLflow

Every training run logs:

| Category | Items |
|----------|-------|
| **Parameters** | architecture, epochs, batch size, LR, dropout, optimizer, augmentation, â€¦ |
| **Metrics (per epoch)** | loss, accuracy, val_loss, val_accuracy, top-3 accuracy, AUC |
| **Test metrics** | accuracy, top-3 accuracy, AUC, macro F1, weighted F1, per-class F1/P/R |
| **Artefacts** | model summary TXT, confusion matrix PNG, per-class metrics bar chart, training curves PNG, ROC curves PNG, precision-recall curves PNG, confidence histogram PNG, classification report TXT |
| **Model** | Keras model â†’ MLflow Model Registry (`cifar10-resnet`) |

---

## ğŸ”„ Airflow DAG

The DAG `cifar10_training_pipeline` (in `dags/`) has **7 tasks** wired with the
TaskFlow API:

```
check_environment
      â”‚
preprocess_data
      â”‚
train_model           â† MLflow run starts here
      â”‚
evaluate_model        â† Loads model from the run's MLflow artefact
      â”‚
register_model        â† Compare vs Production; promote if better
      â”‚
generate_report       â† Self-contained HTML + JSON report saved to disk
      â”‚
notify                â† Prints summary to Airflow task logs
```

### XCom data flow

| Producer | â†’ Consumer | Payload |
|----------|-----------|---------|
| `check_environment` | `preprocess_data` | env info dict |
| `preprocess_data` | `train_model` | dataset stats |
| `train_model` | `evaluate_model`, `register_model`, `generate_report`, `notify` | run_id, metrics |
| `evaluate_model` | `register_model`, `generate_report`, `notify` | full metrics dict |
| `register_model` | `generate_report`, `notify` | version, promoted flag |
| `generate_report` | `notify` | HTML report path |

### Quality gate

Model registration requires `test_accuracy â‰¥ 0.70` (overridable at runtime
via Airflow Variable `cifar10_accuracy_threshold`).

---

## âš™ï¸  Airflow Setup & Run

### Setup (once)

```bash
python scripts/setup_airflow.py
```

### Start all services

```bash
bash scripts/start_services.sh
# MLflow UI  â†’  http://localhost:5000
# Airflow UI â†’  http://localhost:8080  (admin / admin)
```

Or manually in separate terminals:

```bash
# Terminal 1 â€” MLflow
mlflow ui --port 5000

# Terminal 2 â€” Airflow scheduler
export AIRFLOW_HOME="$(pwd)/airflow_home"
export AIRFLOW__CORE__DAGS_FOLDER="$(pwd)/dags"
export AIRFLOW__CORE__LOAD_EXAMPLES=False
airflow scheduler

# Terminal 3 â€” Airflow webserver
export AIRFLOW_HOME="$(pwd)/airflow_home"
airflow webserver --port 8080
```

### Trigger the DAG

```bash
# From CLI
airflow dags trigger cifar10_training_pipeline

# Or via Airflow UI â†’ http://localhost:8080 â†’ DAGs â†’ cifar10_training_pipeline â†’ â–¶ Trigger
```

---

## ğŸ§ª Running Tests

```bash
pytest tests/ -v
# with coverage:
pytest tests/ -v --cov=src --cov-report=term-missing
```

The test suite covers:

- âœ… Config loading & validation
- âœ… Data split sizes, shapes, one-hot encoding, no data leakage
- âœ… Pixel range after normalisation & augmentation shape preservation
- âœ… Model output shapes & softmax constraint (rows sum to 1)
- âœ… All three architectures build without errors
- âœ… Callback creation for all LR scheduler types
- âœ… 2-epoch mini-training smoke test
- âœ… Residual block dimension arithmetic

---

## ğŸ“ˆ Training Configuration Reference

Edit `config/config.yaml` to tune the pipeline:

```yaml
training:
  epochs: 30            # total epochs (early stopping may cut short)
  batch_size: 64
  learning_rate: 0.001
  lr_scheduler: "cosine"      # cosine | step | constant
  early_stopping_patience: 7
  optimizer: "adam"

data:
  validation_split: 0.1       # carved out of training set
  augment: true               # random crop, flip, brightness, contrast, saturation
  random_seed: 42
```

---

## ğŸ”— Comparing Runs

```bash
# Top 10 runs by accuracy
python scripts/compare_runs.py

# Top 5 runs sorted by macro F1
python scripts/compare_runs.py --top 5 --metric macro_f1
```

---

## ğŸ“¦ Model Registry Workflow

```
New run (test_accuracy) â”€â”€â–º < threshold? â”€â”€â–º Rejected (not registered)
                               â”‚
                            â‰¥ threshold
                               â”‚
                         Register version
                               â”‚
                      Better than Production?
                         â”‚               â”‚
                        Yes              No
                         â”‚               â”‚
                    Promote to      Keep in Staging
                    Production      (archive old)
```

---

## ğŸ¯ Expected Results

| Architecture | Epochs | Test Accuracy | Macro F1 |
|---|---|---|---|
| `simple_cnn` | 30 | ~82â€“85% | ~0.82 |
| `resnet_custom` â­ | 30 | **~87â€“91%** | **~0.88** |
| `efficientnet_transfer` | 30 (frozen) | ~85â€“88% | ~0.86 |

> Results vary with random seed, hardware, and exact TensorFlow version.

---

## ğŸ“ Output Artefacts (per run)

```
logs/<run_name>/
â”œâ”€â”€ best_model.keras             # Best checkpoint (val_accuracy)
â”œâ”€â”€ training_curves.png          # Accuracy + loss over epochs
â”œâ”€â”€ confusion_matrix.png         # Normalised confusion matrix
â”œâ”€â”€ per_class_metrics.png        # Precision / Recall / F1 bar chart
â”œâ”€â”€ classification_report.txt    # sklearn full report
â””â”€â”€ evaluation/
    â”œâ”€â”€ roc_curves.png           # Per-class ROC (one-vs-rest)
    â”œâ”€â”€ precision_recall_curves.png
    â”œâ”€â”€ confidence_histogram.png
    â”œâ”€â”€ per_class_metrics.csv
    â””â”€â”€ metrics.json

logs/reports/
â””â”€â”€ report_<run_id>.html         # Self-contained HTML performance report
```

---

## ğŸ›  Extending the Pipeline

| Goal | Where to edit |
|------|--------------|
| Add a new architecture | `src/model.py` â†’ add builder function + register in `builders` dict |
| Change augmentation | `src/data_preprocessing.py` â†’ `_augment()` |
| Add new tracked metrics | `src/train.py` â†’ `_log_detailed_metrics()` |
| Add a new DAG task | `dags/cifar10_training_pipeline.py` â†’ add `@task` and wire into flow |
| Change promotion threshold | Airflow Variable `cifar10_accuracy_threshold` (no code change needed) |
| Send Slack/email notification | `dags/.py` â†’ `notify()` task |
